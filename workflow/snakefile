import os
import pandas as pd

## master snakefile

## setting environmental variables
DATASET_DIR = config["dataset_dir"]
LONGITUDINAL = config["longitudinal_dataset"]
RUN_DEMUX_DADA2 = config["raw_sequences"]
R_SCRIPT_DIR = config["r_script_dir"]
QIIME = config["qiime_env"]

## step 1
RAW_SEQ_DIR = config["raw_seq_dir"]
RAW_SEQS = config["raw_seqs"]
BARCODES = config["barcodes"]
TRIM_LFT_FOR = config["dada2_trim_left_for"]
TRIM_LFT_REV = config["dada2_trim_left_rev"]
TRUNC_LEN_FOR = config["dada2_trunc_len_for"]
TRUNC_LEN_REV = config["dada2_trunc_len_rev"]

## steps 2 and 3
BIOM = config["biom_table"]
REP_SEQS = config["rep_seqs"]

## step 4 and 5
METADATA = config["metadata"]
CORE_SAMPLING_DEPTH = config["core_metrics_sampling_depth"]
PROCESSED_META = config["processed_metadata"]

## step 6 (non-longitudinal)
SAMPLEID_KEY = config["sampleID_key"]
MOUSEID_FACIL_KEY = config["mouseID_facil_key"]
BILE_ACID = config["bile_acid"]
TOXIN = config["toxin"]
HISTO = config["histo"]
METAB = config["metab"]
HYPOXIA = config["hypoxia"]


## defining output file paths from rules
## first need function to do this
def comb_filepaths(filepath1,
                   filepath2):
    return os.path.join(filepath1, filepath2)

proc_meta = pd.read_csv(os.path.join(DATASET_DIR, PROCESSED_META), sep = '\t')


## creating lists of inputs for rule_all
## includes outputs from step 1 (these look funky bc I'm using wildcards)
raw_seq_rule_all = [expand(os.path.join(DATASET_DIR, RAW_SEQ_DIR, "{run}_demux.qza"),
                                        run=RAW_SEQS),
                    expand(os.path.join(DATASET_DIR, RAW_SEQ_DIR, "{run}_demux_details.qza"),
                                        run=RAW_SEQS),
                    expand(os.path.join(DATASET_DIR, RAW_SEQ_DIR, "{run}_demux.qzv"),
                                        run=RAW_SEQS),
                    expand(os.path.join(DATASET_DIR, "data/qiime/{run}_table.qza"),
                                        run=RAW_SEQS),
                    expand(os.path.join(DATASET_DIR, "data/qiime/{run}_rep_seqs.qza"),
                                        run=RAW_SEQS),
                    expand(os.path.join(DATASET_DIR, "data/qiime/{run}_denoise_stats.qza"),
                                        run=RAW_SEQS),
                    os.path.join(DATASET_DIR, "data/qiime/merged_table.qza"),
                    os.path.join(DATASET_DIR, "data/qiime/merged_rep_seqs.qza")]


## includes outputs from steps 2, 3, and 4
base_input = ["databases/sepp-refs-silva-128.qza",
              "databases/silva-138-99-515-806-nb-classifier.qza",
              "data/qiime/tree.qza",
              "data/qiime/placements.qza",
              "data/qiime/filt_table.qza",
              "data/qiime/rem_table.qza",
              "data/qiime/taxonomy.qza",
              "data/qiime/taxonomy_filtered.qza",
              "data/qiime/taxonomy_filtered.qzv",
              "data/qiime/lacto_cecal_table.qza",
              "data/qiime/lacto_rep_seqs.qza",
              "data/qiime/lactoOnly_rep_seqs.fasta",
              "data/qiime/total_sum_scaling.tsv",
              "data/qiime/total_sum_scaling.biom",
              "data/qiime/total_sum_scaling.qza",
              "data/qiime/total_sum_filt_table.qza",
              "data/qiime/total_sum_rem_table.qza",
              "data/qiime/tax_filt.qza",
              "data/qiime/tax_filt.qzv",
              "data/qiime/otu_table.qza",
              "data/qiime/tax_barplot.qzv",
              "data/qiime/core_outputs/unweighted_unifrac_distance_matrix.qza",
              "data/qiime/core_outputs/uw_dist_matrix.tsv",
              "data/qiime/core_outputs/weighted_unifrac_distance_matrix.qza",
              "data/qiime/core_outputs/w_dist_matrix.tsv",
              "data/qiime/core_outputs/shannon_vector.qza",
              "data/qiime/core_outputs/shannon_entropy.tsv",
              "data/qiime/core_outputs/faith_pd_vector.qza",
              "data/qiime/core_outputs/faith_pd.tsv"]


rule_all_input_list = [comb_filepaths(DATASET_DIR, filepath) for filepath in base_input]

## includes outputs from step 5 (longitudinal outputs only)
longitudinal = ["plots/faith_pd.pdf",
                "plots/shannon_entropy.pdf",
                "stats/faith_lm.tsv",
                "stats/faith_dunn.tsv",
                "stats/shannon_lm.tsv",
                "stats/shannon_dunn.tsv",
                "plots/faith_stat_vis.pdf",
                "plots/shannon_stat_vis.pdf",
                "plots/unweighted_unifrac_pcoa.pdf",
                "plots/weighted_unifrac_pcoa.pdf",
                "stats/w_adonis_results.tsv",
                "stats/uw_adonis_results.tsv",
                "plots/family_abun1.pdf",
                "plots/family_abun2.pdf",
                "stats/family_abun_lm.tsv",
                "stats/family_abun_dunn.tsv",
                "plots/famAbun_stat_vis.pdf"]

resil_homog_outList = ["stats/wu_homogeneity.tsv",
                       "stats/wu_homog_dunn.tsv",
                       "stats/uu_homogeneity.tsv",
                       "stats/uu_homog_dunn.tsv",
                       "plots/wu_homogeneity.pdf",
                       "plots/wu_homog_stats.pdf",
                       "plots/uu_homogeneity.pdf",
                       "plots/uu_homog_stats.pdf",
                       "stats/uu_resiliency.tsv",
                       "stats/uu_resil_dunn.tsv",
                       "stats/wu_resiliency.tsv",
                       "stats/wu_resil_dunn.tsv",
                       "plots/wu_resiliency.pdf",
                       "plots/wu_resil_stats.pdf",
                       "plots/uu_resiliency.pdf",
                       "plots/uu_resil_stats.pdf"]

## snakemake gets upset if resil and homog rules aren't run bc the outputs aren't made
## so I'm fixing that w this ifelse statement
if proc_meta.query('day_post_inf == -8').shape[0] > 0 == True:
    longitudinal.append(resil_homog_outList)
else:
    longitudinal

rule_all_longitudinal = [comb_filepaths(DATASET_DIR, filepath) for filepath in longitudinal]

## includes outputs from step 6 (not longitudinal only)
not_longitudinal = ["data/misc/seq_depth.tsv",
                    PROCESSED_META,
                    "data/misc/corrected_bile_acid.tsv",
                    "data/misc/processed_neatToxin.tsv",
                    "data/misc/processed_dilutedToxin.tsv",
                    "data/misc/processed_metabolomics.tsv",
                    "data/misc/processed_histopathology.tsv",
                    "data/misc/processed_bile_acid.tsv",
                    "data/misc/processed_ratio_bileAcid.tsv",
                    "plots/faith_pd.pdf",
                    "plots/shannon_entropy.pdf",
                    "stats/faith_diet_results.tsv",
                    "stats/faith_dunn.tsv",
                    "stats/shannon_diet_results.tsv",
                    "stats/shannon_dunn.tsv",
                    "plots/faith_stat_vis.pdf",
                    "plots/shannon_stat_vis.pdf",
                    "plots/unweighted_unifrac_pcoa.pdf",
                    "plots/weighted_unifrac_pcoa.pdf",
                    "stats/w_adonis_results.tsv",
                    "stats/uw_adonis_results.tsv",
                    "plots/family_abun1.pdf",
                    "plots/family_abun2.pdf",
                    "stats/family_abun_lm.tsv",
                    "stats/family_abun_dunn.tsv",
                    "plots/famAbun_stat_vis.pdf",
                    "plots/histopathology.pdf",
                    "stats/histopathology_lm.tsv",
                    "stats/histopathology_dunn.tsv",
                    "plots/neat_toxin.pdf",
                    "plots/dil_toxin.pdf",
                    "stats/neatToxin_kruskal_test.tsv",
                    "stats/neatToxin_dunn_test.tsv",
                    "stats/dilToxin_kruskal_test.tsv",
                    "stats/dilToxin_dunn_test.tsv",
                    "plots/metabolomics.pdf",
                    "stats/metab_linear_model.tsv",
                    "stats/metab_dunn_test.tsv",
                    "stats/metab_kruskal_test.tsv"]

rule_all_not_longitudinal = [comb_filepaths(DATASET_DIR, filepath) for filepath in not_longitudinal]


## if else statements for which rules to run (include various rule snakemake files here...I think)
if RUN_DEMUX_DADA2 == 'yes' and LONGITUDINAL == 'yes':

    rule_all_input_list.extend(raw_seq_rule_all)

    rule_all_input_list.append(rule_all_longitudinal)

    include: "rules/01_demux_dada2.smk"
    include: "rules/05_longitudinal.smk"

    print("Running demux, DADA2, and longitudinal analysis!")

else:
    print(" ")



if RUN_DEMUX_DADA2 == 'yes' and LONGITUDINAL == 'no':

    rule_all_input_list.extend(raw_seq_rule_all)

    rule_all_input_list.append(rule_all_not_longitudinal)

    include: "rules/01_demux_dada2.smk"
    include: "rules/06_notLongitudinal.smk"

    print("Running demux and DADA2, but NOT longitudinal analysis!")
else:
    print(" ")



if RUN_DEMUX_DADA2 == 'yes' and LONGITUDINAL == 'no downstream':

    rule_all_input_list.extend(raw_seq_rule_all)

    include: "rules/01_demux_dada2.smk"

    print("Running upper qiime analysis ONLY!")

else:
    print(" ")



if RUN_DEMUX_DADA2 == 'no' and LONGITUDINAL == 'yes':

    rule_all_input_list.append(rule_all_longitudinal)

    include: "rules/05_longitudinal.smk"

    print("Running longitudinal-based analysis but not demux and DADA2!")
else: 
    print(" ")




if RUN_DEMUX_DADA2 == 'no' and LONGITUDINAL == 'no':

    rule_all_input_list

    print("Running phylogenetic classification, core metrics, and total sum scaling!")

else:
    print(" ")



## rule all
rule all:
    input:
        data = rule_all_input_list




## loading in needed rules for all
include: "rules/02_phylogeny.smk"
include: "rules/03_tss.smk"
include: "rules/04_core_metrics.smk"


